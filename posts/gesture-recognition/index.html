<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><title>
        Motion & Gesture Recognition for low-res TOF-Sensors
    </title><link href=/images/favicon.png rel=icon type=image/png><link href=/js/temml/Temml-Local.css rel=stylesheet><script src=https://blog.flxzt.net/js/temml/temml.min.js></script><script src=https://blog.flxzt.net/js/temml/auto-render.min.js></script><script src=https://blog.flxzt.net/node_modules/elevator.js/elevator.min.js></script><link href=https://blog.flxzt.net/node_modules/@fontsource/roboto/latin.css rel=stylesheet><link href=https://blog.flxzt.net/node_modules/@fontsource/merriweather/latin.css rel=stylesheet><link href=https://blog.flxzt.net/node_modules/@fontsource/inconsolata/latin.css rel=stylesheet><script async data-goatcounter=https://flxzt.goatcounter.com/count src=https://blog.flxzt.net/js/count.js></script><noscript><img src="https://flxzt.goatcounter.com//count?p=/posts/gesture-recognition/&t=Motion & Gesture Recognition for low-res TOF-Sensors"></noscript><link href=https://blog.flxzt.net/atom.xml rel=alternate title=blog.flxzt.net type=application/atom+xml><link href=https://blog.flxzt.net/theme/light.css rel=stylesheet><link media="(prefers-color-scheme: dark)" href=https://blog.flxzt.net/theme/dark.css rel=stylesheet><link href=https://blog.flxzt.net/main.css media=screen rel=stylesheet><script src=https://blog.flxzt.net/js/main.js></script><body><div class=content><header><div class=site-info><a class=site-title href=https://blog.flxzt.net/>blog.flxzt.net</a><div class=site-description>My personal blog</div></div><nav class=site-menu><a class=site-menu-entry href=/posts>/posts</a><a class=site-menu-entry href=/projects>/projects</a><a class=site-menu-entry href=/tags>/tags</a><a class=site-menu-entry href=/thoughts>/thoughts</a><a class=site-menu-entry href=/about>/about</a></nav><div class=socials><a class=social href=https://blog.flxzt.net/atom.xml> <img alt=rss src=/social_icons/rss.svg> </a><a class=social href=https://github.com/flxzt/> <img alt=github src=/social_icons/github.svg> </a><a class=social href=https://mastodon.social/@flxzt> <img alt=mastodon src=/social_icons/mastodon.svg> </a></div></header><div class=main-content><main><article><div class=page-title>Motion & Gesture Recognition for low-res TOF-Sensors</div><div class=padded-content><div class=meta><time>2022-12-18</time><span class=tags> <small> <a class=tag href=https://blog.flxzt.net/tags/electronics/>electronics</a> </small> <small> <a class=tag href=https://blog.flxzt.net/tags/sensors/>sensors</a> </small> <small> <a class=tag href=https://blog.flxzt.net/tags/math/>math</a> </small> <small> <a class=tag href=https://blog.flxzt.net/tags/embedded/>embedded</a> </small> </span></div><div class=toc><h1>Table of Contents</h1><ul><li><a href=https://blog.flxzt.net/posts/gesture-recognition/#motivation>Motivation</a><li><a href=https://blog.flxzt.net/posts/gesture-recognition/#coordinates>Coordinates</a> <ul><li><a href=https://blog.flxzt.net/posts/gesture-recognition/#conversion>Conversion</a></ul><li><a href=https://blog.flxzt.net/posts/gesture-recognition/#sensor-grid>Sensor Grid</a><li><a href=https://blog.flxzt.net/posts/gesture-recognition/#object-position>Object Position</a><li><a href=https://blog.flxzt.net/posts/gesture-recognition/#motion-and-gesture-recognition>Motion and Gesture Recognition</a><li><a href=https://blog.flxzt.net/posts/gesture-recognition/#swipes>Swipes</a><li><a href=https://blog.flxzt.net/posts/gesture-recognition/#static-holds>Static Holds</a><li><a href=https://blog.flxzt.net/posts/gesture-recognition/#implementation>Implementation</a></ul></div><section><h1 id=motivation>Motivation</h1><p>Low-cost Time-Of-Flight sensors like the "ST VL53L5CX" usually have low resolution SPAD arrays (e.g. 4x4 or 8x8) with individual zones, each measuring a distance to an object located in a section of the FOV of the sensor. These measurement can be processed on a MCU like a STM32 to recognize motion and gestures. For this purpose I created a no-std, no-alloc Rust library with C-bindings to be able to integrate it into C-based embedded codebases, such as the ones created by STM32CubeMX.<br> This blog post explains the algorithms how this library calculates positions and recognizes motions and gestures from the sensor values.<h1 id=coordinates>Coordinates</h1><p>First, let's establish how positions in relation to the sensor are represented as coordinates.<p>$$ C_{cart} = \begin{pmatrix} x: \text{distance to origin on x-axis}\newline y: \text{distance to origin on y-axis}\newline z: \text{distance to origin on z-axis}\newline \end{pmatrix} $$<p>$$ C_{spher} = \begin{pmatrix} r: \text{distance to the origin}\newline \theta \text{ (theta): angle to x-axis (azimuth)}\newline \phi \text{(phi): angle to y-axis (zenith)}\newline \end{pmatrix} $$<p><a href=https://blog.flxzt.net/posts/gesture-recognition/./assets/coordinates.svg><img alt src=https://blog.flxzt.net/posts/gesture-recognition/./assets/coordinates.svg></a><p>This spherical notation is the mathematical convention.<br> (reference: <a href=https://mathworld.wolfram.com/SphericalCoordinates.html>mathworld.wolfram.com/SphericalCoordinates.html</a>)<h2 id=conversion>Conversion</h2><p>Convert between the two: From spherical to cartesian coordinates $$ \begin{align} x &= r \cdot \cos(\theta) \cdot \sin(\phi)\newline y &= r \cdot \sin(\theta) \cdot \sin(\phi)\newline z &= r \cdot \cos(\phi) \end{align} $$<p>and from cartesian to spherial $$ \begin{align} \textcolor{green}{r} &= \sqrt{x^2 + y^2 + z^2}\newline \theta &= \arctan(y / x)\newline \phi &= \arccos( z / \textcolor{green}{r} ) \end{align} $$<h1 id=sensor-grid>Sensor Grid</h1><aside style=max-width:50%><div class=aside-inner><a href=./assets/sensor_grid.svg><img alt src=./assets/sensor_grid.svg></a></div></aside><p>We have for each zone:<p>$$ \begin{align} dist_{i_x, i_y} &: \text{the measured distance}\newline i_x &: \text{horizontal index in the sensor grid}\newline i_y &: \text{vertical index in the sensor grid}\newline \end{align} $$<p>To determine the position of the measurement in each zone, we need to calculate it with the Sensor-FOV:<p>$$ \begin{align} Res_{hor} &: \text{horizontal resolution}\newline Res_{vert} &: \text{vertical resolution}\newline Fov_{hor} &: \text{horizontal sensor field-of-view}\newline Fov_{vert} &: \text{vertical sensor field-of-view} \end{align} $$<div class=clear></div><p>We can determine the zone angle deltas:<p>$$ \begin{align} a_{zone, hor} &= \frac{Fov_{hor}}{Res_{hor}}\newline a_{zone, vert} &= \frac{Fov_{vert}}{Res_{vert}}\newline \end{align} $$ and then we can get the spherical coordinates: $$ C_{spher, i_x, i_y} = \begin{pmatrix} \begin{align} r &= dist_{i_x, i_y}\newline \theta &= (i_x - \frac{Res_{hor}}{2}) \cdot a_{zone, hor}\newline \phi &= \frac{\pi}{2} - (i_y - \frac{Res_{vert}}{2}) \cdot a_{zone, vert}\newline \end{align} \end{pmatrix} $$<h1 id=object-position>Object Position</h1><p>To determine the position of an object in front of the sensor relatively accurately even with the low resolution, we use the following method:<br> We calculate the coordinates for all zones. Then we take the one with the smallest distance $r$ as the initial position $C_{min}$. Because that alone is not very accurate, we need to weigh in the other measurements. We use the weighted average mean:<p>$$ C_{obj} = \frac{ \sum_{i_x = 0, i_y = 0}^{n_x = Res_{hor}, n_y = Res_{vert}} \omega_{i_x, i_y} \cdot C_{i_x, i_y} }{ \sum_{i_x = 0, i_y = 0}^{n_x = Res_{hor}, n_y = Res_{vert}} \omega_{i_x, i_y} } $$<p>Now, how should this weight $\omega_{i_x, i_y}$ look like? Because the object is likely larger than the FOV of a single zone, it should count in neighboring zones as well. The further away a measurement is to $ C_{min} $, the less it should be weighted. With this criteria, we can construct a formula like this:<p>$$ \omega_{i_x, i_y}(C_{min}, C_{i_x, i_y}) = f \cdot \frac{1}{| C_{min} - C_{i_x, i_y}|} $$<p>$f$ is a factor that determines how much the distance weighs in. This can be adjusted based on the size and distance of the to be detected objects and is an entirely subjective value.<h1 id=motion-and-gesture-recognition>Motion and Gesture Recognition</h1><p>Now that we have determined the object position, we save it for every sensor readout in a vector, with the time when it was measured. To be able to reliably recognize gestures, this vector should hold at least ca 2 seconds of position data, so for a 15Hz sensor it should hold at least 30 elements, for a 60Hz sensor it should at least hold 120 elements. The size of course also depends on the memory constraints of the device that runs the library.<br> To avoid accidental recognition of far away objects, detection can be aborted if the distances are above a certain threshold.<h1 id=swipes>Swipes</h1><p>To detect swipes, we look at the measurements newer than a specified time (ca 600ms) and look if the object position has moved horizontally more than a certain distance. To make this more robust, we only recognize the gesture if the object also has not moved much towards or away from the sensor.<p><a href=https://blog.flxzt.net/posts/gesture-recognition/./assets/swipe_gesture.svg><img alt src=https://blog.flxzt.net/posts/gesture-recognition/./assets/swipe_gesture.svg></a><br> <span class=caption> The recognition of a swipe gesture </span><h1 id=static-holds>Static Holds</h1><p>Static holds are determined when the object position has not changed above a distance threshold for a specified amount of time.<p><a href=https://blog.flxzt.net/posts/gesture-recognition/./assets/hold_gesture.svg><img alt src=https://blog.flxzt.net/posts/gesture-recognition/./assets/hold_gesture.svg></a><br> <span class=caption> The recognition of a hold gesture </span><h1 id=implementation>Implementation</h1><p>stay tuned for another post that shows the implementation as a no-std, no-alloc Rust library with integration into an existing STM32 C-based project. It will also show how HID keyboard reports are sent when a gesture is recognized, and how to debug the Rust library on an STM32-F4 MCU.<p>The code (a STM32CubeIDE project) is already available <a href=https://github.com/flxzt/gestures_w_rust>here</a>.</section></div></article></main></div><footer><small> Felix Zwettler - <a href=http://creativecommons.org/licenses/by-sa/4.0/ rel=license><img alt="Creative Commons Lizenzvertrag" src=https://i.creativecommons.org/l/by-sa/4.0/80x15.png></a> - Â© 2025 </small></footer><button class=elevator-button>ðŸ›—</button></div></body><script src=https://blog.flxzt.net/js/temml_config.js></script>